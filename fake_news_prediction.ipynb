{"cells":[{"cell_type":"code","execution_count":1,"source":["import numpy as np\n","import pandas as pd\n","import re # regular expression is used to find a specific text or letter/ word in a sentence or paragraph\n","import nltk\n","#nltk.download('stopwords')\n","from nltk.corpus import stopwords # these are basically the words which don't convey much meaning like a the an etc.\n","from nltk.stem.porter import PorterStemmer # this is used to stem the word like for eg if we have loved --> love!\n","from sklearn.feature_extraction.text import CountVectorizer #to vectorize the words into a vector of frequent words count!\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from nltk.stem import SnowballStemmer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["fake_news_dataset = pd.read_csv('data/complete_dataset.csv')\n","real_news_dataset = pd.read_csv('data/real_complete_dataset.csv',index_col=['Unnamed: 0'])\n","real_news_dataset = real_news_dataset.iloc[:2050,:]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["import numpy as np\n","import pandas as pd\n","import re # regular expression is used to find a specific text or letter/ word in a sentence or paragraph\n","import nltk\n","#nltk.download('stopwords')\n","from nltk.corpus import stopwords # these are basically the words which don't convey much meaning like a the an etc.\n","from nltk.stem.porter import PorterStemmer # this is used to stem the word like for eg if we have loved --> love!\n","from sklearn.feature_extraction.text import CountVectorizer #to vectorize the words into a vector of frequent words count!\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from nltk.stem import SnowballStemmer"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["#print(stopwords.words('spanish'))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["fake_news_dataset = pd.read_csv('data/complete_dataset.csv')\n","real_news_dataset = pd.read_csv('data/real_complete_dataset.csv',index_col=['Unnamed: 0'])\n","real_news_dataset = real_news_dataset.iloc[:2050,:]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["stem = SnowballStemmer('spanish')# basically creating an object for stemming! Stemming is basically getting the root word, for eg: loved --> love!"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["# now let's create a function to preprocess a cell and then apply it to the entire feature!\n","def stemming(content):\n","stemmed_content = re.sub('[^a-zA-Z]', ' ',content) # this basically replaces everything other than lower a-z & upper A-Z with a ' ', for eg apple,bananna --> apple bananna\n","stemmed_content = stemmed_content.lower() # to make all text lower case\n","stemmed_content = stemmed_content.split() # this basically splits the line into words with delimiter as ' '\n","stemmed_content = [stem.stem(word) for word in stemmed_content if not word in stopwords.words('spanish')] # basically remove all the stopwords and apply stemming to the final data\n","stemmed_content = ' '.join(stemmed_content) # this basically joins back and returns the cleaned sentence\n","return stemmed_content"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["fake_news_dataset = pd.read_csv('data/complete_dataset.csv')\n","real_news_dataset = pd.read_csv('data/real_complete_dataset.csv',index_col=['Unnamed: 0'])\n","real_news_dataset = real_news_dataset.iloc[:2050,:]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["del(fake_news_dataset['link'])\n","news_dataset = pd.concat([fake_news_dataset,real_news_dataset])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["stem = SnowballStemmer('spanish')# basically creating an object for stemming! Stemming is basically getting the root word, for eg: loved --> love!"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["# now let's create a function to preprocess a cell and then apply it to the entire feature!\n","def stemming(content):\n","stemmed_content = re.sub('[^a-zA-Z]', ' ',content) # this basically replaces everything other than lower a-z & upper A-Z with a ' ', for eg apple,bananna --> apple bananna\n","stemmed_content = stemmed_content.lower() # to make all text lower case\n","stemmed_content = stemmed_content.split() # this basically splits the line into words with delimiter as ' '\n","stemmed_content = [stem.stem(word) for word in stemmed_content if not word in stopwords.words('spanish')] # basically remove all the stopwords and apply stemming to the final data\n","stemmed_content = ' '.join(stemmed_content) # this basically joins back and returns the cleaned sentence\n","return stemmed_content"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["# let's apply the function on our feature content\n","news_dataset['descripcion_stem'] = news_dataset['titulo'].apply(stemming)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["from sklearn.model_selection import train_test_split\n","training_data, testing_data = train_test_split(news_dataset, test_size=0.2, random_state=0)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["X = training_data['descripcion_stem'].values\n","y = training_data['is_fake'].values"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(X)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["#classifier = LogisticRegression(C = 100, penalty = 'l2', solver= 'newton-cg')\n","classifier = LogisticRegression(C = 0.1, penalty = 'l2', solver= 'liblinear')\n","modelo = classifier.fit(X, y)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["# accuracy score on training data\n","y_pred_train = modelo.predict(X)\n","accuracy_train = accuracy_score(y,y_pred_train)\n","from sklearn.model_selection import cross_val_score\n","accuracies = cross_val_score(estimator = modelo,X = X,y= y , cv = 10)\n","print(\"-------------------------------\")\n","print(\"Accuracy score on training data: \", accuracy_train)\n","print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n","print(\"Standard Deviation: {:.2f}\".format(accuracies.std()*100))"],"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------\n","Accuracy score on training data:  0.9445843828715366\n","Accuracy: 92.98 %\n","Standard Deviation: 1.53\n"]}],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["# grid search to find better hyper parameters\n","#from sklearn.model_selection import GridSearchCV\n","#parameters = [{'solver': ['newton-cg','liblinear'], 'penalty': ['l2'],'C': [100, 10, 1.0, 0.1, 0.01]}]\n","#grid_search = GridSearchCV(estimator=modelo,\n","#                          param_grid=parameters,\n","#                          scoring='accuracy',\n","#                          cv=10)\n","#grid_search.fit(X,y)\n","#print(\"Best Accuracy: {:.2f} %\".format(grid_search.best_score_*100))\n","#print(\"Best Parameters: \", grid_search.best_params_)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":19,"source":["# seperating the data and vectorizing to predict the labels from the model we made!\n","X_test = testing_data['descripcion_stem']\n","X_test = vectorizer.transform(X_test)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":20,"source":["# now to predict the labels from the model!\n","y_pred_final = modelo.predict(X_test)\n","print(y_pred_final)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0\n"," 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0\n"," 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1\n"," 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1\n"," 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0\n"," 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0\n"," 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1\n"," 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0\n"," 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1\n"," 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1\n"," 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1\n"," 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1\n"," 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1\n"," 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0\n"," 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1\n"," 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0\n"," 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0\n"," 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1\n"," 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n"," 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0\n"," 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0\n"," 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["y_pred_final.shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(794,)"]},"metadata":{},"execution_count":21}],"metadata":{}},{"cell_type":"code","execution_count":22,"source":["testing_data['is_fake'].shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(794,)"]},"metadata":{},"execution_count":22}],"metadata":{}},{"cell_type":"code","execution_count":23,"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","# actual values\n","actual = testing_data['is_fake']\n","# predicted values\n","predicted = y_pred_final\n","# confusion matrix\n","matrix = confusion_matrix(actual,predicted, labels=[1,0])\n","print('Matriz de Confusion : \\n',matrix)\n","# outcome values order in sklearn\n","tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n","print('Outcome values : \\n', tp, fn, fp, tn)\n","# classification report for precision, recall f1-score and accuracy\n","matrix = classification_report(actual,predicted,labels=[1,0])\n","print('Reporte de Clasificacion : \\n',matrix)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz de Confusion : \n"," [[373   8]\n"," [ 38 375]]\n","Outcome values : \n"," 373 8 38 375\n","Reporte de Clasificacion : \n","               precision    recall  f1-score   support\n","\n","           1       0.91      0.98      0.94       381\n","           0       0.98      0.91      0.94       413\n","\n","    accuracy                           0.94       794\n","   macro avg       0.94      0.94      0.94       794\n","weighted avg       0.94      0.94      0.94       794\n","\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}